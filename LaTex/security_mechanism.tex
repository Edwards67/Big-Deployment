To enhance the security of our web server environment, we implemented a robust mechanism for gathering and logging IP addresses. This approach involves capturing IP addresses attempting to access our server on ports other than 22 (SSH) and 80 (HTTP) and maintaining comprehensive logs for further analysis. The IP gathering mechanism is designed to identify unauthorized access attempts by capturing IP addresses that try to connect to non-standard ports. This is achieved through a script that utilizes tcpdump to monitor network traffic and filter out unwanted connections~\cite{TechTarget_Net}. The command

\begin{lstlisting}
tcpdump -i any "ip and not host $my_ip and not port 22 and not port 80" -n -tttt > "$temp_file"
\end{lstlisting}

will listen for all connections not on ports 22 and 80, and export them to a temporary file. This is because those are the ports my team will use to connect and test the website, and we don't want to get blocked. The command

\begin{lstlisting}
(grep -oP '(\d{1,3}\.){3}\d{1,3}' "$temp_file" | grep -v "$my_ip" | sort | uniq >> "$cumulative_ip_list" )
\end{lstlisting}

searches the temporary file for any IP addresses and exports them to the IP list. The captured IP addresses are stored in a cumulative list, which can be used to block malicious IPs via iptables. These commands are executed every two minutes via a cron job, which reduces the attackers' window of opportunity and ensures that the temporary file remains manageable in size.

In addition to gathering IP addresses, we implemented an hourly logging mechanism to maintain detailed records of network activity. By excluding traffic on ports 22 and 80, we focus our monitoring on potentially unwanted or malicious attempts. This involves running a script that logs all other IP traffic, storing the data with timestamps~\cite{Schwartz_TCPDUMP}. These logs are crucial for analyzing traffic patterns and identifying potential security threats. The logging process is automated with a cron job that runs every hour. This schedule helps us pinpoint the peak times attackers are trying to breach our defenses and allows us to trace the method and timing of any successful intrusions. The logging gives us the port number they accessed and the IP of the attacker. With this information, we can filter that port and attempt to block them from getting in that way again. To ensure no data is lost during log creation, the tcpdump process is stopped two minutes before the end of each hour, giving the script enough time to write to the timestamped log file. To keep our logging effective, we regularly review the logs. These reviews involve checking the log files to ensure they are accurately capturing data as intended. We also analyze the recorded attempts to understand the strategies attackers are using to target our server. This ongoing evaluation helps us stay informed about new threats, enabling us to adapt our defenses accordingly.
